{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Furubira_District,_Hokkaido.html',\n",
       " 'Valentin_Yanin.html',\n",
       " 'Kings_XI_Punjab_in_2014.html',\n",
       " 'William_Harvey_Lillard.html',\n",
       " 'Radial_Road_3.html',\n",
       " 'George_Weldrick.html',\n",
       " 'Zgornji_Otok.html',\n",
       " 'Blue_Heelers_(season_8).html',\n",
       " 'Taggen_Nunatak.html',\n",
       " '1951_National_League_tie-breaker_series.html']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(\"wiki\")\n",
    "print(len(files))\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Radial Road 3 - Wikipedia</title>\n",
      "<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );</script>\n",
      "<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Radial_Road_3\",\"wgTitle\":\"Radial Road 3\",\"wgCurRevisionId\":750120640,\"wgRevisionId\":750120640,\"wgArticleId\":22602027,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Infobox road temporary tracking category 1\",\"Routes in Metro Manila\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgMonthNamesShort\":[\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],\"wgRelevantPageName\":\"Radial_Road_3\",\"wgRelevantArticleId\":22602027,\"wgRequestId\":\"WLNQCwpAAEAAAUk54qkAAAAC\",\"wgIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgFlaggedRevsParams\":{\"tags\":{}},\"wgStableRevisionId\":null,\"wgWikiEditorEnabledModules\":{\"toolbar\":true,\"dialogs\":true,\"preview\":false,\"publish\":false},\"wgBetaFeaturesFeatures\":[],\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsShouldSendModuleToUser\":false,\"wgPopupsConflictsWithNavPopupGadget\":false,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"usePageImages\":true,\"usePageDescriptions\":true},\"wgPreferredVariant\":\"en\",\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"nearby\":true,\"watchlist\":true,\"tagline\":true},\"wgRelatedArticles\":null,\"wgRelatedArticlesBetaFeatureEnabled\":false,\"wgRelatedArticlesUseCirrusSearc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"wiki/Radial_Road_3.html\") as f:\n",
    "    print(f.read()[:2000])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17531657218933105 tiempo de lectura simple\n"
     ]
    }
   ],
   "source": [
    "# Simple reading:\n",
    "\n",
    "def simple_read_files(files):\n",
    "    content = []\n",
    "    for file in files:\n",
    "        with open(\"wiki/\"+file,\"r\") as f:\n",
    "            content.append(f.read())\n",
    "    return content \n",
    "\n",
    "start1 = time.time()\n",
    "simple_read_files(files)\n",
    "end1 = time.time()\n",
    "        \n",
    "print(end1 - start1, \"tiempo de lectura simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Furubira_District,_Hokkaido',\n",
       " 'Valentin_Yanin',\n",
       " 'Kings_XI_Punjab_in_2014',\n",
       " 'William_Harvey_Lillard',\n",
       " 'Radial_Road_3',\n",
       " 'George_Weldrick',\n",
       " 'Zgornji_Otok',\n",
       " 'Blue_Heelers_(season_8)',\n",
       " 'Taggen_Nunatak',\n",
       " '1951_National_League_tie-breaker_series']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = [file.replace(\".html\",\"\") for file in files]\n",
    "articles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35270142555236816 tiempo con dos threads\n"
     ]
    }
   ],
   "source": [
    "def read_files(filename):\n",
    "    with open(\"wiki/\"+filename,\"r\") as f:\n",
    "        content = f.read()\n",
    "        return content\n",
    "\n",
    "start1 = time.time()\n",
    "pool = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n",
    "contents = list(pool.map(read_files,files))\n",
    "end1 = time.time()\n",
    "print(end1 - start1, \"tiempo con dos threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We gains to little with threads because the task is computationally intensive and the GIRL of Cython does not allow parallelization in code-execution (one thread at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.344219446182251 1  workers\n",
      "0.2848210334777832 2  workers\n",
      "0.28457093238830566 3  workers\n",
      "0.3295416831970215 4  workers\n",
      "0.3415031433105469 5  workers\n",
      "0.35746288299560547 6  workers\n"
     ]
    }
   ],
   "source": [
    "# Testing optimal amount of workers for reading content of \n",
    "# files. \n",
    "\n",
    "for i in range(1,7):\n",
    "    start2 = time.time()\n",
    "    pool = concurrent.futures.ThreadPoolExecutor(max_workers=i)\n",
    "    content2 = list(pool.map(read_files,files))\n",
    "    end2 = time.time()\n",
    "    print(end2 - start2, i, \" workers\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The efficiency changes for every execution. This is because we are working with few cores (less or equal than 2), but the performance is better than simple reading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the content of html files. \n",
    "\n",
    "def parse_html(html_content):\n",
    "    parser = BeautifulSoup(html_content, 'html.parser')\n",
    "    return str(parser.find_all(\"div\",id=\"content\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.28571963310242 1  workers\n",
      "35.53234243392944 2  workers\n",
      "35.80071258544922 3  workers\n",
      "38.409151554107666 4  workers\n"
     ]
    }
   ],
   "source": [
    "# Using processes\n",
    "\n",
    "del(pool)\n",
    "for i in range(1,5):\n",
    "    start2 = time.time()\n",
    "    pool = concurrent.futures.ProcessPoolExecutor(max_workers=i)\n",
    "    parsed = list(pool.map(parse_html,content2))\n",
    "    end2 = time.time()\n",
    "    print(end2 - start2, i, \" workers\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since the time is variable for different reasons (like limited amount of CPU's), there is no significativelly gains in use different amount of workers,  each increment impacts in computation performance. But nevertheless it was suggested to use 3 workers because of the availables hardware capabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Let's count up how many times each tag occurs. , if there are a lot of a tags on each page, we know that Wikipedia articles tend to be very connected to other articles or pages. On the other hand, a lot of div tags will tell us that Wikipedia pages tend to have a nested structure with many page elements.\n",
    "\n",
    "##### Since the task is computationally intense, we gone a use Processes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.765032529830933\n"
     ]
    }
   ],
   "source": [
    "def count_tags(html): \n",
    "    parser = BeautifulSoup(html,\"html.parser\")\n",
    "    tags = {}\n",
    "    for tag in parser.find_all(): \n",
    "        if tag.name not in tags: \n",
    "            tags[tag.name] = 1\n",
    "        tags[tag.name] += 1\n",
    "    return tags\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=3)\n",
    "dictionaries = list(pool.map(count_tags,parsed))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s': 20, 'ul': 12080, 'p': 9001, 'bdi': 10, 'strong': 994, 'rp': 66, 'mrow': 6, 'span': 68366, 'div': 29738, 'del': 6, 'h6': 4, 'img': 7707, 'big': 120, 'q': 99, 'dt': 406, 'small': 3719, 'br': 5714, 'mo': 6, 'rb': 34, 'area': 60, 'abbr': 4416, 'dl': 566, 'caption': 341, 'i': 19200, 'ruby': 34, 'h3': 983, 'blockquote': 89, 'pre': 4, 'rt': 34, 'wbr': 124, 'h5': 10, 'cite': 4160, 'b': 15455, 'hr': 70, 'sub': 180, 'math': 6, 'table': 4964, 'mstyle': 6, 'code': 162, 'annotation': 6, 'h2': 5020, 'samp': 6, 'source': 6, 'sup': 11984, 'center': 78, 'td': 58825, 'semantics': 6, 'li': 87029, 'tr': 28464, 'h4': 150, 'u': 64, 'font': 82, 'th': 15396, 'a': 162452, 'noscript': 2000, 'map': 6, 'ol': 1658, 'dd': 1464, 'audio': 6, 'h1': 2000}\n"
     ]
    }
   ],
   "source": [
    "# Combine all dictionaries in a single one to get trends \n",
    "# in trends in all the wiki html files.\n",
    "\n",
    "combined_tags = {}\n",
    "for dic in dictionaries: \n",
    "    for k,v in dic.items():\n",
    "        if k not in combined_tags:\n",
    "            combined_tags[k] = v\n",
    "        combined_tags[k] += v\n",
    "        \n",
    "print(combined_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Counting words in every html-file\n",
    "\n",
    "def counts_words(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    words = {}\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(\"\\W+\", \" \", text.lower()) \n",
    "# remember that \\w+ matches one or more word characters (same as [a-zA-Z0-9_]+).\n",
    "    words = text.split(\" \")\n",
    "    words = [w for w in words if len(w) >= 5] \n",
    "    return dict(Counter(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.409634590148926 Taking the time for the task\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=3)\n",
    "dictionaries2 = list(pool.map(counts_words,parsed))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start, \"Taking the time for the task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75653 words\n"
     ]
    }
   ],
   "source": [
    "# Combining dictionaries of word counts\n",
    "\n",
    "combined_words = {}\n",
    "for dic in dictionaries2: \n",
    "    for k,v in dic.items():\n",
    "        if k not in combined_words:\n",
    "            combined_words[k] = v\n",
    "        combined_words[k] += v\n",
    "print(len(combined_words), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding the most common words in every html-file\n",
    "\n",
    "def most_common_words(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    words = {}\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(\"\\W+\", \" \", text.lower()) \n",
    "# remember that \\w+ matches one or more word characters (same as [a-zA-Z0-9_]+).\n",
    "    words = text.split(\" \")\n",
    "    words = [w for w in words if len(w) >= 5] \n",
    "    return dict(Counter(words).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.40528702735901 Taking the time for the task\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=3)\n",
    "dictionaries2 = list(pool.map(most_common_words,parsed))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start, \"Taking the time for the task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4766 words\n"
     ]
    }
   ],
   "source": [
    "# Combining dictionaries of word counts\n",
    "\n",
    "combined_words = {}\n",
    "for dic in dictionaries2: \n",
    "    for k,v in dic.items():\n",
    "        if k not in combined_words:\n",
    "            combined_words[k] = v\n",
    "        combined_words[k] += v\n",
    "print(len(combined_words), \"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Next steps for data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
